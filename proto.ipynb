{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training videos\n",
    "import cv2\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "from PIL import Image\n",
    "\n",
    "NUM_FRAMES = 100\n",
    "SEQ_LENGTH = 10\n",
    "TAKES_PER = 3\n",
    "MOODS = ['RELAXED', 'EXCITED']\n",
    "HIDDEN_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get ready to act: RELAXED\n",
      "Recording started\n",
      "Recording stopped\n",
      "Get ready to act: EXCITED\n",
      "Recording started\n",
      "Recording stopped\n",
      "Get ready to act: RELAXED\n",
      "Recording started\n",
      "Recording stopped\n",
      "Get ready to act: EXCITED\n",
      "Recording started\n",
      "Recording stopped\n",
      "Get ready to act: RELAXED\n",
      "Recording started\n",
      "Recording stopped\n",
      "Get ready to act: EXCITED\n",
      "Recording started\n",
      "Recording stopped\n"
     ]
    }
   ],
   "source": [
    "# Photo studio\n",
    "# Here you can record movies to use as training data\n",
    "# Get ready to act either excited or relaxed!\n",
    "# Videos are stored in data dir\n",
    "def capture(num_frames, path='out.avi', countdown=0):\n",
    "    for i in reversed(range(max(0, countdown))):\n",
    "        i = 'GO!' if i == 0 else '{}  '.format(i)\n",
    "        sys.stdout.write(\"{}   \\r\".format(i))\n",
    "        sys.stdout.flush()\n",
    "        sleep(1)\n",
    "\n",
    "    # Create a VideoCapture object\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # Check if camera opened successfully\n",
    "    if (cap.isOpened() == False): \n",
    "        print(\"Unable to read camera feed\")\n",
    "\n",
    "    # Default resolutions of the frame are obtained.The default resolutions are system dependent.\n",
    "    # We convert the resolutions from float to integer.\n",
    "    frame_width = int(cap.get(3))\n",
    "    frame_height = int(cap.get(4))\n",
    "\n",
    "    # Define the codec and create VideoWriter object.The output is stored in 'outpy.avi' file.\n",
    "    out = cv2.VideoWriter(path, cv2.VideoWriter_fourcc('M','J','P','G'), 10, (frame_width,frame_height))\n",
    "\n",
    "    print('Recording started')\n",
    "    for i in range(num_frames):\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if ret == True:     \n",
    "            # Write the frame into the file 'output.avi'\n",
    "            out.write(frame)\n",
    "\n",
    "\n",
    "    # When everything done, release the video capture and video write objects\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print('Recording stopped')\n",
    "    \n",
    "for take in range(TAKES_PER):\n",
    "    for mood in MOODS:\n",
    "        path = 'data/{}{}.avi'.format(mood, take)\n",
    "        print('Get ready to act:', mood)\n",
    "        capture(NUM_FRAMES, path=path, countdown=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing data/RELAXED2.avi\n",
      "preprocessing data/RELAXED1.avi\n",
      "preprocessing data/RELAXED0.avi\n",
      "preprocessing data/EXCITED1.avi\n",
      "preprocessing data/EXCITED0.avi\n",
      "preprocessing data/EXCITED2.avi\n"
     ]
    }
   ],
   "source": [
    "# Process videos, create training data\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "\n",
    "class VGGFramePreprocessor():\n",
    "    \n",
    "    def __init__(self, vgg_model):\n",
    "        self.vgg_model = vgg_model\n",
    "    \n",
    "    def process(self, frame):\n",
    "        img_data = cv2.resize(frame,(224,224))\n",
    "        img_data = np.expand_dims(img_data, axis=0)\n",
    "        img_data = preprocess_input(img_data)\n",
    "        x = self.vgg_model.predict(img_data).flatten()\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        return x\n",
    "\n",
    "def get_video_frames(video_path):\n",
    "    vidcap = cv2.VideoCapture(video_path)\n",
    "    success, frame = vidcap.read()\n",
    "    while success:\n",
    "        yield frame\n",
    "        success,frame = vidcap.read()\n",
    "    vidcap.release()\n",
    "    \n",
    "frame_preprocessor = VGGFramePreprocessor(VGG16(weights='imagenet', include_top=False))\n",
    "\n",
    "# Load movies and transform frames to features\n",
    "movies = []\n",
    "for mood in MOODS:\n",
    "    y = np.array([0,1]) if mood == 'EXCITED' else np.array([1,0])\n",
    "    for video_path in glob.glob('data/{}*.avi'.format(mood)):\n",
    "        print('preprocessing', video_path)\n",
    "        X = [frame_preprocessor.process(frame) for frame in get_video_frames(video_path)]\n",
    "        X = np.concatenate(X)\n",
    "        movies.append({\n",
    "            'X': X,\n",
    "            'y': y\n",
    "        })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display an image\n",
    "img = Image.fromarray(movies[0]['X'][0].reshape(49,512))\n",
    "img.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(540, 10, 25088)\n",
      "(540, 2)\n",
      "[[ 0.         0.         0.        ... 24.366207  15.429358   0.       ]\n",
      " [ 0.         0.         0.        ... 33.379276  16.243172   0.       ]\n",
      " [ 0.         0.         0.        ... 36.367126  16.402796   0.       ]\n",
      " ...\n",
      " [ 0.         0.         0.        ... 35.094444  15.89079    0.       ]\n",
      " [ 0.         0.         0.        ... 32.163406  15.844413   1.9477578]\n",
      " [ 0.         0.         0.        ... 28.701618  14.477264   0.       ]]\n",
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "# Create windows from movies\n",
    "X, y = [], []\n",
    "for movie in movies:\n",
    "    movie_X = movie['X']\n",
    "    sequences = []\n",
    "    for i in range(len(movie_X) - SEQ_LENGTH):\n",
    "        sequence = movie_X[i: i + SEQ_LENGTH]\n",
    "        X.append(np.expand_dims(sequence, axis=0))\n",
    "        y.append(np.expand_dims(movie['y'], axis=0))\n",
    "\n",
    "\n",
    "X = np.concatenate(X)\n",
    "y = np.concatenate(y)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X[0])\n",
    "print(y[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 364 samples, validate on 41 samples\n",
      "Epoch 1/5\n",
      "364/364 [==============================] - 5s 13ms/step - loss: 0.3501 - acc: 0.9176 - val_loss: 0.1499 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "364/364 [==============================] - 3s 9ms/step - loss: 0.1126 - acc: 1.0000 - val_loss: 0.0375 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "364/364 [==============================] - 3s 9ms/step - loss: 0.0395 - acc: 1.0000 - val_loss: 0.0110 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "364/364 [==============================] - 3s 9ms/step - loss: 0.0172 - acc: 1.0000 - val_loss: 0.0038 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "364/364 [==============================] - 3s 9ms/step - loss: 0.0097 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb280125f8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "HIDDEN_SIZE_LSTM = 16\n",
    "HIDDEN_SIZE_DENSE = 16\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(HIDDEN_SIZE_LSTM, return_sequences=True, input_shape=(SEQ_LENGTH, X.shape[2])))\n",
    "#model.add(LSTM(HIDDEN_SIZE_LSTM, return_sequences=True))\n",
    "model.add(LSTM(HIDDEN_SIZE_LSTM))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=10, epochs=5,\n",
    "          validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 1.0\n"
     ]
    }
   ],
   "source": [
    "y_true = [np.argmax(y) for y in y_test]\n",
    "y_pred = [np.argmax(pred) for pred in model.predict(x_test)]\n",
    "score = f1_score(y_true, y_pred)\n",
    "print('F1:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame 199  hash: 5555158754670880279 . relaxed: 0.0014%  excited: 0.9986%  mood: EXCITED    \r"
     ]
    }
   ],
   "source": [
    "# Infer on live video\n",
    "from collections import deque\n",
    "from math import ceil\n",
    "\n",
    "test_frames = 200\n",
    "buffer = deque(maxlen=SEQ_LENGTH)\n",
    "\n",
    "# Initialize camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Check if camera opened successfully\n",
    "if (cap.isOpened() == False): \n",
    "    print(\"Unable to read camera feed\")\n",
    "    test_frames = 0\n",
    "\n",
    "# Start processing video\n",
    "for i in range(test_frames):\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        processed = frame_preprocessor.process(frame)\n",
    "        buffer.append(processed)\n",
    "    # enough data in buffer?\n",
    "    if len(buffer) == SEQ_LENGTH:\n",
    "        # predict\n",
    "        x_buffer = np.expand_dims(np.concatenate(buffer), axis=0)\n",
    "        prediction = model.predict(np.array(x_buffer))\n",
    "        # compute a hash\n",
    "        buffer_hash = hash(bytes(x_buffer))\n",
    "        relaxed = prediction[0][0]\n",
    "        excited = prediction[0][1]\n",
    "        mood = MOODS[np.argmax(prediction[0])]\n",
    "        progress = i\n",
    "        message = 'frame {}  hash: {} . relaxed: {:.04f}%  excited: {:.04f}%  mood: {}   \\r'.format(\n",
    "            progress, buffer_hash, relaxed, excited, mood)\n",
    "    else:\n",
    "        # buffer more\n",
    "        progress = ceil(100*i/SEQ_LENGTH)\n",
    "        message = 'buffering {}%         \\r'.format(progress)\n",
    "    sys.stdout.write(message)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "cap.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (p3-ai)",
   "language": "python",
   "name": "p3-ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
